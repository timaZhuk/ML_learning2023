{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba876af4",
   "metadata": {},
   "source": [
    "# Basic Gradient Descent Algorithm\n",
    "**The gradient descent** algorithm is an *approximate and iterative method* for mathematical optimization. You can use it to approach the minimum of any differentiable function.\n",
    "* Although gradient descent sometimes gets stuck in a local minimum or a saddle point instead of finding the global minimum, it’s widely used in practice. Data science and machine learning methods often apply it internally to optimize model parameters. For example, neural networks find weights and biases with gradient descent.\n",
    "**************************************\n",
    "* The **cost function, or loss function**, is the function to be minimized (or maximized) by varying the decision variables. \n",
    "* Many machine learning methods solve optimization problems under the surface. **They tend to minimize the difference between actual and predicted outputs by adjusting the model parameters** (like weights and biases for neural networks, decision rules for random forest or gradient boosting, and so on).\n",
    "* In a **regression problem**, you typically have the vectors of **input variables 𝐱 = (𝑥₁, …, 𝑥ᵣ)** and the actual **outputs 𝑦**. You want to find a model that maps 𝐱 to a predicted response 𝑓(𝐱) so that 𝑓(𝐱) is as close as possible to 𝑦. \n",
    "**For example, you might want to predict an *output such as a person’s salary* given inputs like the person’s number of years at the company or level of education.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2203c",
   "metadata": {},
   "source": [
    "## Minimization of residuals\n",
    "In this type of problem, you want to **minimize the sum of squared residuals (SSR)**, where **SSR = Σᵢ(𝑦ᵢ − 𝑓(𝐱ᵢ))²** for all observations 𝑖 = 1, …, 𝑛, where **𝑛 is the total number of observations**. Alternatively, you could use the mean squared error **(MSE = SSR / 𝑛) instead of SSR.**\n",
    "* Both SSR and MSE use the square of the difference between the actual and predicted outputs. The lower the difference, the more accurate the prediction. A difference of zero indicates that the prediction is equal to the actual data.\n",
    "* In a **classification problem**, the outputs **𝑦 are categorical**, often either 0 or 1. \n",
    "* For example, you might try to predict whether an email is spam or not. In the case of binary outputs, it’s convenient to minimize the **cross-entropy function** that also depends on the actual outputs **𝑦ᵢ and the corresponding predictions 𝑝(𝐱ᵢ)**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af918e5e",
   "metadata": {},
   "source": [
    "## Gradient of a Function: Calculus Refresher\n",
    "In calculus, the derivative of a function shows you how much a value changes when you modify its argument (or arguments). \n",
    "* Derivatives are important for optimization because the zero derivatives might indicate a minimum, maximum, or saddle point.\n",
    "* The gradient of a function 𝐶 of several independent variables 𝑣₁, …, 𝑣ᵣ is denoted with **∇𝐶(𝑣₁, …, 𝑣ᵣ) and defined as the vector function of the partial derivatives of 𝐶 with respect to each independent variable: ∇𝐶 = (∂𝐶/∂𝑣₁, …, ∂𝐶/𝑣ᵣ). The symbol ∇ is called nabla**.\n",
    "* The nonzero value of the gradient of a function **𝐶** at a given point defines the direction and rate of the fastest increase of 𝐶. When working with gradient descent, you’re interested in the direction of the fastest decrease in the cost function. This direction is determined by the negative gradient, −∇𝐶."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c776c0",
   "metadata": {},
   "source": [
    "## Intuition Behind Gradient Descent\n",
    "To understand the **gradient descent algorithm**, imagine a drop of water sliding down the side of a bowl or a ball rolling down a hill. The drop and the ball **tend to move in the direction of the fastest decrease** until they reach the bottom. With time, they’ll gain momentum and accelerate.\n",
    "* The idea behind gradient descent is similar: \n",
    "* you start with an arbitrarily chosen position of the point or vector 𝐯 = (𝑣₁, …, 𝑣ᵣ) and move it iteratively in the direction of the fastest decrease of the cost function. As mentioned, this is the direction of the negative gradient vector, −∇𝐶.\n",
    "* Once you have a random starting point **𝐯 = (𝑣₁, …, 𝑣ᵣ)**, you update it, or move it to a new position in the direction of the **negative gradient: 𝐯 → 𝐯 − 𝜂∇𝐶, where 𝜂 (pronounced “ee-tah”) is a small positive value called the learning rate**.\n",
    "* The learning rate determines how large the update or moving step is. It’s a very important parameter. If 𝜂 is too small, then the algorithm might converge very slowly. Large 𝜂 values can also cause issues with convergence or make the algorithm divergent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebb9f4",
   "metadata": {},
   "source": [
    "## Implementation of Basic Gradient Descent\n",
    "Now that you know how the basic gradient descent works, you can implement it in Python. You’ll use only plain Python and NumPy, which enables you to write concise code when working with arrays (or vectors) and gain a performance boost.\n",
    "\n",
    "This is a basic implementation of the algorithm that starts with an arbitrary point, start, iteratively moves it toward the minimum, and returns a point that is hopefully at or near the minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab3919e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ndef gradient_descent(gradient, start, learning_rate, n_iter):\\n    vector= start\\n    for _ in range(n_iter):\\n        diff=-learning_rate*gradient(vector)\\n        vector+=diff\\n    return vector\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(gradient, start, learning_rate, n_iter=50, tolerance=1e-06):\n",
    "    vector= start\n",
    "    for _ in range(n_iter):\n",
    "        diff=-learning_rate*gradient(vector)\n",
    "        print(diff)\n",
    "        if np.all(np.abs(diff)<=tolerance):\n",
    "            break\n",
    "        vector+=diff\n",
    "    return vector\n",
    "\n",
    "\n",
    "''' \n",
    "def gradient_descent(gradient, start, learning_rate, n_iter):\n",
    "    vector= start\n",
    "    for _ in range(n_iter):\n",
    "        diff=-learning_rate*gradient(vector)\n",
    "        vector+=diff\n",
    "    return vector\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc8529",
   "metadata": {},
   "source": [
    "gradient_descent() takes four arguments:\n",
    "\n",
    "* **gradient** is the function or any Python callable object that takes a vector and returns the gradient of the function you’re trying to minimize.\n",
    "* **start** is the point where the algorithm starts its search, given as a sequence (tuple, list, NumPy array, and so on) or scalar (in the case of a one-dimensional problem).\n",
    "* **learn_rate** is the learning rate that controls the magnitude of the vector update.\n",
    "* **n_iter** is the number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3098357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.0\n",
      "-2.4000000000000004\n",
      "-1.44\n",
      "-0.8639999999999999\n",
      "-0.5184\n",
      "-0.31104\n",
      "-0.18662399999999996\n",
      "-0.11197439999999997\n",
      "-0.06718463999999998\n",
      "-0.04031078399999999\n",
      "-0.02418647039999999\n",
      "-0.01451188223999999\n",
      "-0.008707129343999994\n",
      "-0.005224277606399997\n",
      "-0.0031345665638399982\n",
      "-0.001880739938303999\n",
      "-0.0011284439629823991\n",
      "-0.0006770663777894395\n",
      "-0.00040623982667366374\n",
      "-0.00024374389600419823\n",
      "-0.00014624633760251893\n",
      "-8.774780256151136e-05\n",
      "-5.2648681536906814e-05\n",
      "-3.158920892214409e-05\n",
      "-1.895352535328645e-05\n",
      "-1.137211521197187e-05\n",
      "-6.8232691271831225e-06\n",
      "-4.093961476309873e-06\n",
      "-2.4563768857859235e-06\n",
      "-1.473826131471554e-06\n",
      "-8.842956788829324e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.210739197207331e-06"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for function: f(v) = 2*v\n",
    "'''\n",
    "The updates are larger at first because the value of the gradient (and slope) \n",
    "is higher. As you approach the minimum, they become lower.\n",
    "'''\n",
    "gradient_descent(gradient=lambda v:2*v, start=10.0, learning_rate=0.2)\n",
    "#You get a result that’s very close to zero, which is the correct minimum.2.210739197207331e-06\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6364f12",
   "metadata": {},
   "source": [
    "**The learning rate** is a very important parameter of the algorithm. Different learning rate values can significantly affect the behavior of gradient descent. Consider the previous example, but with a learning rate of 0.8 instead of 0.2:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6fb1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16.0\n",
      "9.600000000000001\n",
      "-5.7600000000000025\n",
      "3.4560000000000017\n",
      "-2.073600000000001\n",
      "1.2441600000000008\n",
      "-0.7464960000000005\n",
      "0.44789760000000034\n",
      "-0.26873856000000024\n",
      "0.16124313600000015\n",
      "-0.09674588160000011\n",
      "0.058047528960000074\n",
      "-0.03482851737600005\n",
      "0.02089711042560003\n",
      "-0.01253826625536002\n",
      "0.007522959753216013\n",
      "-0.004513775851929608\n",
      "0.002708265511157765\n",
      "-0.0016249593066946595\n",
      "0.0009749755840167959\n",
      "-0.0005849853504100776\n",
      "0.00035099121024604663\n",
      "-0.00021059472614762804\n",
      "0.00012635683568857685\n",
      "-7.581410141314611e-05\n",
      "4.548846084788767e-05\n",
      "-2.729307650873261e-05\n",
      "1.6375845905239567e-05\n",
      "-9.825507543143742e-06\n",
      "5.895304525886246e-06\n",
      "-3.5371827155317477e-06\n",
      "2.1223096293190486e-06\n",
      "-1.2733857775914292e-06\n",
      "7.640314665548576e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4.77519666596786e-07"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(gradient=lambda v:2*v, start=10.0, learning_rate=0.8)\n",
    "#-4.77519666596786e-07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1445f2",
   "metadata": {},
   "source": [
    "* **Small learning rates** can result in very slow convergence. If the number of iterations is limited, then the algorithm may return before the minimum is found. Otherwise, the whole **process might take an unacceptably large amount of time**. To illustrate this, run gradient_descent() again, this time with a much smaller learning rate of 0.005:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccedb1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1\n",
      "-0.099\n",
      "-0.09801\n",
      "-0.0970299\n",
      "-0.096059601\n",
      "-0.09509900498999999\n",
      "-0.09414801494009999\n",
      "-0.09320653479069899\n",
      "-0.092274469442792\n",
      "-0.09135172474836407\n",
      "-0.09043820750088044\n",
      "-0.08953382542587163\n",
      "-0.08863848717161292\n",
      "-0.08775210229989679\n",
      "-0.08687458127689782\n",
      "-0.08600583546412884\n",
      "-0.08514577710948755\n",
      "-0.08429431933839268\n",
      "-0.08345137614500876\n",
      "-0.08261686238355868\n",
      "-0.0817906937597231\n",
      "-0.08097278682212586\n",
      "-0.0801630589539046\n",
      "-0.07936142836436555\n",
      "-0.07856781408072189\n",
      "-0.07778213593991468\n",
      "-0.07700431458051553\n",
      "-0.07623427143471037\n",
      "-0.07547192872036326\n",
      "-0.07471720943315964\n",
      "-0.07397003733882804\n",
      "-0.07323033696543976\n",
      "-0.07249803359578535\n",
      "-0.07177305325982751\n",
      "-0.07105532272722924\n",
      "-0.07034476949995694\n",
      "-0.06964132180495738\n",
      "-0.0689449085869078\n",
      "-0.06825545950103871\n",
      "-0.06757290490602834\n",
      "-0.06689717585696806\n",
      "-0.06622820409839837\n",
      "-0.06556592205741438\n",
      "-0.06491026283684023\n",
      "-0.06426116020847183\n",
      "-0.06361854860638712\n",
      "-0.06298236312032325\n",
      "-0.062352539489120014\n",
      "-0.06172901409422882\n",
      "-0.06111172395328653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.050060671375367"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(gradient=lambda v:2*v, start=10.0, learning_rate=0.005)\n",
    "#6.050060671375367"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ad4cc",
   "metadata": {},
   "source": [
    "* Nonconvex functions might have local minima or saddle points where the algorithm can get trapped. In such situations, your choice of learning rate or starting point can make the difference between finding a local minimum and finding the global minimum.\n",
    "\n",
    "* Consider the function 𝑣⁴ - 5𝑣² - 3𝑣. It has a global minimum in 𝑣 ≈ 1.7 and a local minimum in 𝑣 ≈ −1.42. The gradient of this function is 4𝑣³ − 10𝑣 − 3. Let’s see how gradient_descent() works here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4991381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6000000000000001\n",
      "1.6272\n",
      "-3.783877654118399\n",
      "0.5044141623808883\n",
      "-0.5724248641051475\n",
      "0.7814611222194309\n",
      "-0.6068051171358235\n",
      "0.13899854405416542\n",
      "-0.21933357470903447\n",
      "0.40659134038280453\n",
      "-0.512203314093079\n",
      "0.8309066419990444\n",
      "-0.5927042282201039\n",
      "-0.010794554305831029\n",
      "0.018624689049109477\n",
      "-0.03164506367910996\n",
      "0.0551589320741865\n",
      "-0.09180972031286459\n",
      "0.1642440220053022\n",
      "-0.2546266051894012\n",
      "0.47392968883074116\n",
      "-0.5563977507400388\n",
      "0.8093871859703192\n",
      "-0.5995588383773605\n",
      "0.05422752033237935\n",
      "-0.09030868279849216\n",
      "0.1614643938483404\n",
      "-0.2508143142228118\n",
      "0.46669786160872384\n",
      "-0.5522318570231806\n",
      "0.8143769422625383\n",
      "-0.5980673152424665\n",
      "0.03910363283275196\n",
      "-0.06569153789715934\n",
      "0.11627529928372127\n",
      "-0.18630338952422465\n",
      "0.3431911258959634\n",
      "-0.45954995172682234\n",
      "0.7997672743633807\n",
      "-0.6022665996699983\n",
      "0.08342730411430282\n",
      "-0.13651236828834143\n",
      "0.2480948860367132\n",
      "-0.3607726118573421\n",
      "0.6650510437524251\n",
      "-0.61634491697414\n",
      "0.4732762031978663\n",
      "-0.5560270478614424\n",
      "0.8098641486595959\n",
      "-0.5994188308279139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.4207567437458342"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(lambda v:4*v**3-10*v-3, start=0, learning_rate=0.2)\n",
    "#-1.4207567437458342 it is local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53326dbf",
   "metadata": {},
   "source": [
    "* During the first two iterations, your vector was moving toward the global minimum, but then it crossed to the opposite side and stayed trapped in the local minimum. You can prevent this with a smaller learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aaa2eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000000000000004\n",
      "0.5892000000000001\n",
      "0.9079721326848\n",
      "-0.2246503833085825\n",
      "0.3170935301464246\n",
      "-0.5092435441203079\n",
      "0.6282931870635216\n",
      "-0.9331071597134741\n",
      "0.877865133868027\n",
      "-0.7281732126576643\n",
      "0.7894937239177465\n",
      "-0.9565479152265817\n",
      "0.8842180064032775\n",
      "-0.6890538011762836\n",
      "0.7657933178303029\n",
      "-0.9736646813502603\n",
      "0.8883816572758083\n",
      "-0.6590859161151155\n",
      "0.7461364688680563\n",
      "-0.980552137217972\n",
      "0.889944474732042\n",
      "-0.6467086893975079\n",
      "0.7376372900231133\n",
      "-0.9818043486714143\n",
      "0.8902216937631375\n",
      "-0.6444392423491373\n",
      "0.736054759287217\n",
      "-0.9819325953803504\n",
      "0.8902499654476403\n",
      "-0.6442064847909773\n",
      "0.7358920296145733\n",
      "-0.9819439712289406\n",
      "0.8902524721499709\n",
      "-0.6441858355807981\n",
      "0.7358775891632569\n",
      "-0.981944964473275\n",
      "0.890252691006009\n",
      "-0.6441840326409551\n",
      "0.7358763282979661\n",
      "-0.981945051072647\n",
      "0.8902527100876507\n",
      "-0.6441838754453632\n",
      "0.7358762183648088\n",
      "-0.9819450586221742\n",
      "0.8902527117511424\n",
      "-0.6441838617414238\n",
      "0.735876208781095\n",
      "-0.9819450592803189\n",
      "0.8902527118961605\n",
      "-0.6441838605467567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.285401330315467"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(gradient=lambda v: 4*v**3-10*v-3, start=0, learning_rate=0.1)\n",
    "#1.285401330315467, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84ae3b",
   "metadata": {},
   "source": [
    "* A lower learning rate prevents the vector from making large jumps, and in this case, the vector remains closer to the global optimum.\n",
    "\n",
    "* Adjusting the learning rate is tricky. You can’t know the best value in advance. There are many techniques and heuristics that try to help with this. In addition, machine learning practitioners often tune the learning rate during model selection and evaluation.\n",
    "\n",
    "* Besides the learning rate, the starting point can affect the solution significantly, especially with nonconvex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d95986",
   "metadata": {},
   "source": [
    "# Short Examples\n",
    "* First, you’ll apply gradient_descent() to another one-dimensional problem. Take the function 𝑣 − log(𝑣). The gradient of this function is 1 − 1/𝑣. With this information, you can find its minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f649b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3\n",
      "-0.2727272727272727\n",
      "-0.24056603773584906\n",
      "-0.20356434636701076\n",
      "-0.16287794135027678\n",
      "-0.12128797889960596\n",
      "-0.0829776212093471\n",
      "-0.051970845099847396\n",
      "-0.030087534924710224\n",
      "-0.016413141886228\n",
      "-0.008612682995240983\n",
      "-0.004417914476034346\n",
      "-0.0022382763465940703\n",
      "-0.0011266585386080497\n",
      "-0.0005652340195307359\n",
      "-0.00028309633404838275\n",
      "-0.00014166839365387096\n",
      "-7.086430314817704e-05\n",
      "-3.5439684376137315e-05\n",
      "-1.7721726167096996e-05\n",
      "-8.861334175769287e-06\n",
      "-4.43078487311066e-06\n",
      "-2.2154218842773687e-06\n",
      "-1.1077183043606276e-06\n",
      "-5.538609927357996e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0000011077232125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(gradient = lambda v:1-1/v, start=2.5, learning_rate=0.5)\n",
    "# 1.0000011077232125, this function has the minimum in 𝑣 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6598cf",
   "metadata": {},
   "source": [
    "* The application is the same, but you need to provide the gradient and starting points as vectors or arrays. For example, you can find the minimum of the function 𝑣₁² + 𝑣₂⁴ that has the gradient vector (2𝑣₁, 4𝑣₂³):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18a25ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4 -0.8]\n",
      "[-0.24   -0.0064]\n",
      "[-0.144      -0.00580505]\n",
      "[-0.0864     -0.00529836]\n",
      "[-0.05184    -0.00486244]\n",
      "[-0.031104   -0.00448404]\n",
      "[-0.0186624  -0.00415297]\n",
      "[-0.01119744 -0.00386125]\n",
      "[-0.00671846 -0.00360259]\n",
      "[-0.00403108 -0.00337191]\n",
      "[-0.00241865 -0.00316513]\n",
      "[-0.00145119 -0.00297888]\n",
      "[-0.00087071 -0.00281041]\n",
      "[-0.00052243 -0.0026574 ]\n",
      "[-0.00031346 -0.00251793]\n",
      "[-0.00018807 -0.00239036]\n",
      "[-0.00011284 -0.00227331]\n",
      "[-6.77066378e-05 -2.16560289e-03]\n",
      "[-4.06239827e-05 -2.06621120e-03]\n",
      "[-2.43743896e-05 -1.97426106e-03]\n",
      "[-1.46246338e-05 -1.88899059e-03]\n",
      "[-8.77478026e-06 -1.80973579e-03]\n",
      "[-5.26486815e-06 -1.73591553e-03]\n",
      "[-3.15892089e-06 -1.66701922e-03]\n",
      "[-1.89535254e-06 -1.60259659e-03]\n",
      "[-1.13721152e-06 -1.54224917e-03]\n",
      "[-6.82326913e-07 -1.48562315e-03]\n",
      "[-4.09396148e-07 -1.43240342e-03]\n",
      "[-2.45637689e-07 -1.38230849e-03]\n",
      "[-1.47382613e-07 -1.33508620e-03]\n",
      "[-8.84295679e-08 -1.29051003e-03]\n",
      "[-5.30577407e-08 -1.24837604e-03]\n",
      "[-3.18346444e-08 -1.20850009e-03]\n",
      "[-1.91007867e-08 -1.17071561e-03]\n",
      "[-1.14604720e-08 -1.13487152e-03]\n",
      "[-6.87628320e-09 -1.10083055e-03]\n",
      "[-4.12576992e-09 -1.06846770e-03]\n",
      "[-2.47546195e-09 -1.03766895e-03]\n",
      "[-1.48527717e-09 -1.00833005e-03]\n",
      "[-8.91166303e-10 -9.80355562e-04]\n",
      "[-5.34699782e-10 -9.53657943e-04]\n",
      "[-3.20819869e-10 -9.28156758e-04]\n",
      "[-1.92491921e-10 -9.03777995e-04]\n",
      "[-1.15495153e-10 -8.80453438e-04]\n",
      "[-6.92970917e-11 -8.58120128e-04]\n",
      "[-4.15782550e-11 -8.36719871e-04]\n",
      "[-2.49469530e-11 -8.16198805e-04]\n",
      "[-1.49681718e-11 -7.96507009e-04]\n",
      "[-8.98090308e-12 -7.77598161e-04]\n",
      "[-5.38854185e-12 -7.59429219e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8.08281277e-12, 9.75207120e-02])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(gradient = lambda v: np.array([2*v[0], 4*v[1]**3]), start = np.array([1.0, 1.0]),\n",
    "                 learning_rate = 0.2, tolerance=1e-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20005cc",
   "metadata": {},
   "source": [
    "* The most basic form of linear regression is simple linear regression. It has only one set of inputs 𝑥 and two weights: 𝑏₀ and 𝑏₁. The equation of the regression line is 𝑓(𝑥) = 𝑏₀ + 𝑏₁𝑥. Although the optimal values of 𝑏₀ and 𝑏₁ can be calculated analytically, you’ll use gradient descent to determine them.\n",
    "\n",
    "* First, you need calculus to find the gradient of the cost function 𝐶 = Σᵢ(𝑦ᵢ − 𝑏₀ − 𝑏₁𝑥ᵢ)² / (2𝑛). Since you have two decision variables, 𝑏₀ and 𝑏₁, the gradient ∇𝐶 is a vector with two components:\n",
    "\n",
    "* ∂𝐶/∂𝑏₀ = (1/𝑛) Σᵢ(𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ) = mean(𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ)\n",
    "* ∂𝐶/∂𝑏₁ = (1/𝑛) Σᵢ(𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ) 𝑥ᵢ = mean((𝑏₀ + 𝑏₁𝑥ᵢ − 𝑦ᵢ) 𝑥ᵢ)\n",
    "* You need the values of 𝑥 and 𝑦 to calculate the gradient of this cost function. Your gradient function will have as inputs not only 𝑏₀ and 𝑏₁ but also 𝑥 and 𝑦. This is how it might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fffba388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssr_gradient(x,y, b):\n",
    "    res = b[0]+b[1]*x-y\n",
    "    return res.mean(), (res*x).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fa595",
   "metadata": {},
   "source": [
    "* ssr_gradient() takes the arrays x and y, which contain the observation inputs and outputs, and the array b that holds the current values of the decision variables 𝑏₀ and 𝑏₁. \n",
    "* This function first calculates the array of the residuals for each observation (res) and then returns the pair of values of ∂𝐶/∂𝑏₀ and ∂𝐶/∂𝑏₁."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd5c9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(\n",
    "    gradient, x, y, start, learning_rate=0.1, n_iter=50, tolerance=1e-6\n",
    "):\n",
    "    vector = start\n",
    "    for _ in range(n_iter):\n",
    "        diff = -learning_rate*np.array(gradient(x,y, vector))\n",
    "        if np.all(np.abs(diff)<=tolerance):\n",
    "            break\n",
    "        vector +=diff\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a398df0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.62822349, 0.54012867])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([5, 15, 25, 35, 45, 55])\n",
    "y = np.array([5,20, 14, 32, 22, 38])\n",
    "\n",
    "gradient_descent(\n",
    "    ssr_gradient, x, y, start=[0.5, 0.5], learning_rate=0.0008,\n",
    "    n_iter=100_000\n",
    ")\n",
    "\n",
    "# array([5.62822349, 0.54012867])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "823cf9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(\n",
    "    gradient, x, y, start, learning_rate=0.1, n_iter=50, tolerance=1e-06,\n",
    "    dtype=\"float64\"\n",
    "):\n",
    "    #Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "    \n",
    "    # Setting up the data type for Numpy arrays\n",
    "    dtype_=np.dtype(dtype)\n",
    "    \n",
    "    # Converting x and y to Numpy arrays\n",
    "    x,y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    if x.shape[0] !y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "        \n",
    "    #Initializing the values of the variables\n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "    \n",
    "    #Setting up and checking the learning rate\n",
    "    learning_rate = np.array(learning_rate, dtype=dtype_)\n",
    "    if np.any(learning_rate <= 0):\n",
    "        raise ValueError(\"'learning_rate' must be greater then zero\")\n",
    "    #Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater thean zero\")\n",
    "    \n",
    "    # Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance<=0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "    \n",
    "    # Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        # Recalculating the difference\n",
    "        diff = -lerning_rate*np.array(gradient(x,y, vector), dtype_ )\n",
    "    \n",
    "        #Checking if the absolute difference is small enough\n",
    "        if np.all(np.abs(diff)<=tolerance):\n",
    "        break\n",
    "    \n",
    "        # Updating the values of the variables\n",
    "        vector += diff\n",
    "        \n",
    "    return vector if vector.shape else vector.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad077995",
   "metadata": {},
   "source": [
    "# Minibatches in Stochastic Gradient Descent\n",
    "\n",
    "* You’ll create a new function called sgd() that is very similar to gradient_descent() but uses randomly selected minibatches to move along the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6317d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(\n",
    "    gradient, x, y, start, learning_rate=0.1, batch_size=1, n_iter=50,\n",
    "    tolerance=1e-06, dtype=\"float64\", random_state=None\n",
    "):\n",
    "    # Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "    \n",
    "    #setting up the data type for Numpy arrays\n",
    "    dtype_=np.dtype(dtype)\n",
    "    \n",
    "    #converting x and y toNumPy arrays\n",
    "    x,y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    n_obs = x.shape[0]\n",
    "    if n_obs !=y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
    "    #print(f'xy:  {xy}')\n",
    "    \n",
    "    #Initializing the random number generator\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    vector = np.array(start, dtype=dtype_)\n",
    "    \n",
    "    #Setting up and checking the learning rate\n",
    "    learning_rate = np.array(learning_rate, dtype=dtype_)\n",
    "    if np.any(learning_rate <= 0):\n",
    "        raise ValueError(\"'learning_rate' must be greater than zero\")\n",
    "    \n",
    "    #Setting up and checking the size of minibatches\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0<batch_size<=n_obs:\n",
    "        raise ValueError(\n",
    "            \"'batch_size' must be greater than zero and less than \"\n",
    "            \"or equal to the number of observations\"\n",
    "        )\n",
    "    #Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "        \n",
    "    #Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <=0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "    \n",
    "    #Perfoming the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        #Shuffle x and y\n",
    "        rng.shuffle(xy)\n",
    "        \n",
    "        #performing minibatch moves\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            x_batch, y_batch = xy[start:stop,:-1], xy[start:stop,:-1]\n",
    "            \n",
    "            #recalculating the difference\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
    "            diff = -learning_rate*grad\n",
    "            \n",
    "            #Checking if the absolute difference is small enough\n",
    "            if np.all(np.abs(diff)<=tolerance):\n",
    "                break\n",
    "                \n",
    "            #Updating the values of the variables\n",
    "            vector += diff\n",
    "    return vector if vector.shape else vector.item()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c343cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.61682107e-04, 9.99995934e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd(\n",
    "    ssr_gradient, x, y, start=[0.5, 0.5], learning_rate=0.0008,\n",
    "    batch_size=3, n_iter=100_000, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb57333a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8562a2e0",
   "metadata": {},
   "source": [
    "# Scientific world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b02094d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "c1=abs(-7)\n",
    "c2=abs(3+4j)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94479be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# print(min([2, 45, 13])) # 2\n",
    "# print(min(\"abYcz\")) #Y\n",
    "print(min('1','a','A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4f064a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "z\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "print(max([2, 45, 13]))\n",
    "print(max(\"abYcz\")) #z\n",
    "print(max('1','a','A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "919b736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "6\n",
      "{1, 'b', 'a'}\n"
     ]
    }
   ],
   "source": [
    "print(len([2,7,1]))\n",
    "print(len(\"abcdef\"))\n",
    "print({1,'a','b'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db193755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list=[1,2,3]\n",
    "sum(my_list)\n",
    "my_tuple=(4,5,6)\n",
    "sum(my_tuple)\n",
    "sum_set={7,8,9}\n",
    "sum(sum_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9a12748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min = 4, max = 6, average = 5.0\n"
     ]
    }
   ],
   "source": [
    "def funMinMaxAvg(arr):\n",
    "    print(f'min = {min(arr)}, max = {max(arr)}, average = { sum(arr)/len(arr)}')\n",
    "\n",
    "funMinMaxAvg([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af43635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'moly': 2.13, 'hand_grenade': 3.21, 'grail': 2113.69}\n",
      "{'moly': 2.13, 'hand_grenade': 3.21, 'grail': 2113.69}\n"
     ]
    }
   ],
   "source": [
    "holy={\"moly\":1.99, \"hand_grenade\":3, \"grail\":1975.41}\n",
    "tax_prices=holy\n",
    "for item, price in tax_prices.items():\n",
    "    #Add a 7 percent tax, rounded to the nearest xent\n",
    "    tax_prices[item] = round(1.07*price,2)\n",
    "print(tax_prices)\n",
    "print(holy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57015674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
